{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel, mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tpot import TPOTRegressor\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'train'\n",
    "predictors_paths = {\n",
    "    'GM': os.path.join(train_data_path, 'GM.csv'),\n",
    "    'WM': os.path.join(train_data_path, 'WM.csv'),\n",
    "    'ReHo': os.path.join(train_data_path, 'ReHo.csv'),\n",
    "    'PCGcorr': os.path.join(train_data_path, 'PCGcorr.csv'),\n",
    "    'FA': os.path.join(train_data_path, 'FA.csv'),\n",
    "    'MD': os.path.join(train_data_path, 'MD.csv')\n",
    "}\n",
    "additional_variables_path = os.path.join(train_data_path, 'Subjects.csv')\n",
    "\n",
    "# Predictors\n",
    "modalities = ['GM', 'ReHo', 'MD']\n",
    "selected_modalities = {modality: predictors_paths[modality] for modality in modalities if modality in predictors_paths}\n",
    "def read_and_rename(modality, path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.rename(columns={label: f\"{label}_{modality}\" for label in df.columns if label != 'ID'})\n",
    "    return df\n",
    "dfs = [read_and_rename(modality, path) for modality, path in selected_modalities.items()]\n",
    "predictors_df = reduce(lambda left, right: pd.merge(left, right, on='ID'), dfs)\n",
    "\n",
    "# Response and confounding variables\n",
    "additional_variables = ['Memory', 'Age', 'Sex', 'EducationYear']\n",
    "df = pd.read_csv(additional_variables_path)\n",
    "selected_variables = [variable for variable in additional_variables if variable in df.columns]\n",
    "additional_variables_df = df[[\"ID\"] + selected_variables]\n",
    "\n",
    "# Merge predictors with response and confounding variables on 'ID'\n",
    "df = pd.merge(additional_variables_df, predictors_df, on='ID')\n",
    "\n",
    "# Prepare X and y\n",
    "X = df.drop(columns=['ID', 'Memory'])\n",
    "y = df['Memory']\n",
    "\n",
    "# Split into training and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f'Sample size for training: {X_train.shape[0]}')\n",
    "print(f'Sample size for test: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoML: TPOT (Tree-based Pipeline Optimization Tool)\n",
    "$ \\text{Total pipelines evaluated} = \\text{population size} + (\\text{generations} \\times \\text{offspring size}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://epistasislab.github.io/tpot/using/\n",
    "# generations: number of iterations to run the pipeline optimization process\n",
    "# population_size: number of individuals to retain in the genetic programming (GP) population every generation\n",
    "# offspring_size: number of offspring to produce in each GP generation; by default, offspring_size = population_size\n",
    "# total pipelines evaluated = 50 + (5 x 50) = 300\n",
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2, random_state=42, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.fit(X_train, y_train)\n",
    "predictions = tpot.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f\"MAE by TPOT: {mae:.3f}\")\n",
    "best_pipeline = tpot.fitted_pipeline_\n",
    "print(\"Best pipeline steps:\")\n",
    "for step in best_pipeline.steps:\n",
    "    print(step)\n",
    "tpot.export('tpot_best_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = best_pipeline.steps[-1][1]\n",
    "features = X_train.columns if isinstance(X_train, pd.DataFrame) else range(X_train.shape[1])\n",
    "if hasattr(regressor, 'feature_importances_'):\n",
    "    feature_importances = regressor.feature_importances_\n",
    "elif hasattr(regressor, 'coef_'):\n",
    "    feature_importances = regressor.coef_\n",
    "features_and_importances = zip(features, feature_importances)\n",
    "sorted_features_and_importances = sorted(features_and_importances, key=lambda x: x[1], reverse=True)\n",
    "top_features_and_importances = sorted_features_and_importances[:9]\n",
    "print(f\"Top features' importances by TPOT:\")\n",
    "for no, feature_importance in enumerate(top_features_and_importances):\n",
    "    print(f\"{no + 1}. {feature_importance[0]}: {feature_importance[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP (SHapley Additive exPlanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(regressor, X_train)\n",
    "shap_values = explainer(X_test) # (16, 171)\n",
    "shap.initjs()\n",
    "shap.summary_plot(shap_values, X_test, max_display=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = 'test'\n",
    "predictors_paths = {\n",
    "    'GM': os.path.join(test_data_path, 'GM.csv'),\n",
    "    'WM': os.path.join(test_data_path, 'WM.csv'),\n",
    "    'ReHo': os.path.join(test_data_path, 'ReHo.csv'),\n",
    "    'PCGcorr': os.path.join(test_data_path, 'PCGcorr.csv'),\n",
    "    'FA': os.path.join(test_data_path, 'FA.csv'),\n",
    "    'MD': os.path.join(test_data_path, 'MD.csv')\n",
    "}\n",
    "additional_variables_path = os.path.join(test_data_path, 'Subjects.csv')\n",
    "\n",
    "# Predictors\n",
    "selected_modalities = {modality: predictors_paths[modality] for modality in modalities if modality in predictors_paths}\n",
    "dfs = [read_and_rename(modality, path) for modality, path in selected_modalities.items()]\n",
    "predictors_df = reduce(lambda left, right: pd.merge(left, right, on='ID'), dfs)\n",
    "\n",
    "# Confounding variables\n",
    "df = pd.read_csv(additional_variables_path)\n",
    "selected_variables = [variable for variable in additional_variables if variable in df.columns]\n",
    "additional_variables_df = df[['ID'] + selected_variables]\n",
    "\n",
    "# Merge predictors with confounding variables on 'ID'\n",
    "df = pd.merge(additional_variables_df, predictors_df, on='ID')\n",
    "\n",
    "# Apply trained model\n",
    "X_ext = df.drop(columns=['ID'])\n",
    "predictions_ext = regressor.predict(X_ext)\n",
    "\n",
    "# Save predictions\n",
    "np.savetxt(os.path.join(test_data_path, \"Predictions.txt\"), predictions_ext)\n",
    "\n",
    "# SHAP\n",
    "shap_values = explainer(X_ext) # (10, 171)\n",
    "shap.initjs()\n",
    "shap.summary_plot(shap_values, X_ext, max_display=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
